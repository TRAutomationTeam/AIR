import requests
import yaml
import os
import json
from typing import Dict, List, Any
import logging
import re

class AICodeAnalyzer:
    def __init__(self, config: dict = None, metrics: dict = None):
        self.config = config or {}
        self.metrics = metrics or {}
        
    def analyze_workflow_results(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Use local TinyLlama (via Ollama) for enhanced analysis and AI suggestions"""
        import subprocess
        import sys
        violations = analysis_results.get('rules_violations', [])
        if not violations:
            violations = analysis_results.get('original_analysis', {}).get('rules_violations', [])
        violations = [v for v in violations if v.get('Severity') in ('Warning', 'Error')]

        prompt_lines = [
            "You are an expert UiPath code reviewer. Here are the warnings and errors found in my UiPath workflows:",
            ""
        ]
        for v in violations:
            rule_id = v.get('RuleId', 'Unknown')
            rule_name = v.get('RuleName', '')
            severity = v.get('Severity', '')
            recommendation = v.get('Recommendation', '')
            file = v.get('File', v.get('FilePath', ''))
            prompt_lines.append(f"- [{severity}] {rule_id} ({rule_name}) in {file}: {recommendation}")
        prompt_lines.append("")
        prompt_lines.append("For each issue above, provide a specific, actionable suggestion to resolve it. Format your response as a numbered list, mapping each suggestion to the corresponding issue.")
        detailed_prompt = "\n".join(prompt_lines)

        # Call TinyLlama via Ollama (must be running locally)
        try:
            OLLAMA_PATH = r"C:\Users\s.sp.dev-agentuser\AppData\Local\Programs\Ollama\ollama app.exe"
            result = subprocess.run([
                OLLAMA_PATH, "run", "tinyllama", detailed_prompt
            ], capture_output=True, text=True, timeout=120)
            if result.returncode == 0:
                ai_text = result.stdout.strip()
                # Parse suggestions into a list
                suggestions = [line.strip() for line in ai_text.split('\n') if line.strip()]
                # Use the same reporting structure
                return {
                    'original_analysis': analysis_results,
                    'ai_insights': suggestions,
                    'recommendations': suggestions,
                    'quality_score': 0,
                    'go_no_go_decision': 'REVIEW_REQUIRED',
                    'confidence': 0,
                    'summary': 'AI suggestions generated by TinyLlama',
                    'critical_issues': []
                }
            else:
                logging.error(f"TinyLlama (Ollama) call failed: {result.stderr}")
                return self._fallback_analysis(analysis_results)
        except Exception as e:
            logging.error(f"Error running TinyLlama (Ollama): {str(e)}")
            return self._fallback_analysis(analysis_results)

    def _process_ai_results(self, ai_results: Dict, original_results: Dict) -> Dict[str, Any]:
        """Process AI analysis results and combine with original analysis"""
        insights = []
        recommendations = []
        improvements = []
        quality_score = 0
        decision = 'REVIEW_REQUIRED'
        confidence = 0
        summary = ''
        critical_issues = []

        result = ai_results.get('result', {})
        answer = result.get('answer', {})
        model_data = answer.get(self.model_name, {}) if self.model_name in answer else answer

        def split_suggestions(text):
            if not isinstance(text, str):
                return text
            items = re.split(r'\n\s*\d+\.\s*', text)
            items = [i.strip() for i in items if i.strip()]
            if len(items) > 1:
                return items
            return [line.strip() for line in text.split('\n') if line.strip()]

        if isinstance(model_data, dict):
            insights = model_data.get('insights', [])
            recommendations = model_data.get('recommendations', [])
            improvements = model_data.get('improvements', [])
            quality_score = model_data.get('quality_score', 0)
            decision = model_data.get('decision', 'REVIEW_REQUIRED')
            confidence = model_data.get('confidence', 0)
            summary = model_data.get('summary', '')
            critical_issues = model_data.get('critical_issues', [])
            
            if isinstance(insights, str):
                insights = split_suggestions(insights)
            if isinstance(recommendations, str):
                recommendations = split_suggestions(recommendations)
            if isinstance(improvements, str):
                improvements = split_suggestions(improvements)
        elif isinstance(model_data, str):
            insights = split_suggestions(model_data)

        if not insights:
            insights = ai_results.get('insights', [])
            if isinstance(insights, str):
                insights = split_suggestions(insights)
        if not recommendations:
            recommendations = ai_results.get('recommendations', [])
            if isinstance(recommendations, str):
                recommendations = split_suggestions(recommendations)
        if not improvements:
            improvements = ai_results.get('improvements', [])
            if isinstance(improvements, str):
                improvements = split_suggestions(improvements)

        return {
            'original_analysis': original_results,
            'ai_insights': insights,
            'recommendations': recommendations,
            'quality_score': quality_score,
            'go_no_go_decision': decision,
            'confidence': confidence,
            'summary': summary,
            'critical_issues': critical_issues
        }

    def _fallback_analysis(self, analysis_results: Dict) -> Dict:
        """Return local analysis results when AI analysis fails"""
        logging.info("Using fallback analysis (local results only)")
        return {
            'original_analysis': analysis_results,
            'ai_insights': [],
            'recommendations': [],
            'quality_score': 0,
            'go_no_go_decision': 'REVIEW_REQUIRED',
            'confidence': 0,
            'summary': 'Analysis based on local rules only',
            'critical_issues': []
        }
